{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Help Chat\n",
    "\n",
    "**Use natural language to query Help Centre**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch Data from Web Source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Function to clean text from web pages\n",
    "#\n",
    "\n",
    "import re\n",
    "\n",
    "def clean_text(text: str):\n",
    "    # Normalize line breaks to \\n\\n (two new lines)\n",
    "    text = text.replace(\"\\r\\n\", \"\\n\\n\")\n",
    "    text = text.replace(\"\\r\", \"\\n\\n\")\n",
    "\n",
    "    # Replace two or more spaces with a single space\n",
    "    text = re.sub(\" {2,}\", \" \", text)\n",
    "\n",
    "    # Remove leading spaces before removing trailing spaces\n",
    "    text = re.sub(\"^[ \\t]+\", \"\", text, flags=re.MULTILINE)\n",
    "\n",
    "    # Remove trailing spaces before removing empty lines\n",
    "    text = re.sub(\"[ \\t]+$\", \"\", text, flags=re.MULTILINE)\n",
    "\n",
    "    # Remove empty lines\n",
    "    text = re.sub(\"^\\s+\", \"\", text, flags=re.MULTILINE)\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Function to calculate the number of tokens in a text string.\n",
    "#\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "\"\"\"\n",
    "Function to calculate the number of tokens in a text string.\n",
    "\"\"\"\n",
    "\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "def num_tokens_from_string(string: str) -> int:\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get data files\n",
    "\n",
    "We load a number of HTML pages using the LangChain WebBaseLoader. Each of those pages contains lots of superfluous content so we extract only the relevant article context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.macquarie.com.au//help/personal/home-loans/apply-for-a-home-loan/choosing-which-home-loan-is-right-for-you.html']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#\n",
    "# Build a list of URLs to scrape from a text file.\n",
    "#\n",
    "\n",
    "# read urls from txt file\n",
    "with open('help_urls.txt') as f:\n",
    "    urls = f.readlines()\n",
    "\n",
    "# For debugging - Overide urls from text file \n",
    "urls = [\"/help/personal/home-loans/apply-for-a-home-loan/choosing-which-home-loan-is-right-for-you.html\"]\n",
    "\n",
    "\n",
    "# remove whitespace characters like `\\n` at the end of each line\n",
    "urls = [x.strip() for x in urls]\n",
    "\n",
    "# prepend \"https://www.macquarie.com.au/\" to each url\n",
    "urls = [\"https://www.macquarie.com.au/\" + x for x in urls]\n",
    "\n",
    "display(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data\n",
      "Cleaning Data for 1 documents\n",
      "Doc:   0    Tokens:    341\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Scrape web data from the URLs\n",
    "#\n",
    "\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "import re\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "print(\"Loading Data\")\n",
    "url_loaders = WebBaseLoader(urls, header_template=headers)\n",
    "data = url_loaders.load()\n",
    "\n",
    "#\n",
    "# Extract only the actual Article content from the web page and clean\n",
    "#\n",
    "print(f\"Cleaning Data for {len(data)} documents\")\n",
    "for i, d in enumerate(data):\n",
    "    d.page_content = \"\"\n",
    "    #source = \n",
    "    thedoc = WebBaseLoader(d.metadata['source'], header_template=headers).scrape()\n",
    "    # extract only the Container Article content from the web page\n",
    "    td = thedoc.findAll('div', class_='parsys')\n",
    "    for t in td:\n",
    "        if len(t['class']) == 1 and t['class'][0] == 'parsys':\n",
    "            d.page_content = clean_text(t.text)\n",
    "            data[i] = d\n",
    "            print (f\"Doc: {i:3d}    Tokens: {num_tokens_from_string(d.page_content):6d}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the data into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting Data\n",
      "Number of chunks: 1\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "CHUNK_SIZE = 500\n",
    "\n",
    "# Chunk the data\n",
    "print(\"Splitting Data\")\n",
    "text_splitter = TokenTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=50)\n",
    "docs = text_splitter.split_documents(data)\n",
    "print(f\"Number of chunks: {len(docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Vector Store\n",
    "\n",
    "#### Astra DB Connectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "ASTRA_DB_KEYSPACE = os.environ['ASTRA_DB_KEYSPACE']\n",
    "ASTRA_DB_SECURE_BUNDLE_PATH = os.environ['ASTRA_DB_SECURE_BUNDLE_PATH']\n",
    "ASTRA_DB_APPLICATION_TOKEN = os.environ['ASTRA_DB_APPLICATION_TOKEN']\n",
    "OPENAI_API_KEY = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cassandra.cluster import Cluster\n",
    "from cassandra.auth import PlainTextAuthProvider\n",
    "\n",
    "\n",
    "def getCluster():\n",
    "    \"\"\"\n",
    "    Create a Cluster instance to connect to Astra DB.\n",
    "    Uses the secure-connect-bundle and the connection secrets.\n",
    "    \"\"\"\n",
    "    cloud_config = {\"secure_connect_bundle\": ASTRA_DB_SECURE_BUNDLE_PATH}\n",
    "    auth_provider = PlainTextAuthProvider(\"token\", ASTRA_DB_APPLICATION_TOKEN)\n",
    "    return Cluster(cloud=cloud_config, auth_provider=auth_provider)\n",
    "\n",
    "\n",
    "def get_astra():\n",
    "    \"\"\"\n",
    "    This function is used by LangChain Vectorstore.\n",
    "    \"\"\"\n",
    "    cluster = getCluster()\n",
    "    astraSession = cluster.connect()\n",
    "    return astraSession, ASTRA_DB_KEYSPACE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup Vector Store\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Cassandra\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "# define Embedding model\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Set up the vector store\n",
    "print(\"Setup Vector Store\")\n",
    "session, keyspace = get_astra()\n",
    "vectorstore = Cassandra(\n",
    "    embedding=embeddings,\n",
    "    session=session,\n",
    "    keyspace=keyspace,\n",
    "    table_name=\"helpcentre_db\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store data and embeddings in Astra DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Adding texts to Vector Store\")\n",
    "texts, metadatas = zip(*((doc.page_content, doc.metadata) for doc in docs))\n",
    "vectorstore.add_texts(texts=texts, metadatas=metadatas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_vector",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
