{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Vector Database with data\n",
    "\n",
    "- Crawl website\n",
    "- Extract relevant content section from HTML\n",
    "- Split into chunks\n",
    "- Generate vectors using Embedding model\n",
    "- Store vectors in Astra DB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch Data from Web Source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "#\n",
    "# Function to clean text from web pages\n",
    "#\n",
    "\n",
    "def clean_text(text: str):\n",
    "    # Normalize line breaks to \\n\\n (two new lines)\n",
    "    text = text.replace(\"\\r\\n\", \"\\n\\n\")\n",
    "    text = text.replace(\"\\r\", \"\\n\\n\")\n",
    "\n",
    "    # Replace two or more spaces with a single space\n",
    "    text = re.sub(\" {2,}\", \" \", text)\n",
    "\n",
    "    # Remove leading spaces before removing trailing spaces\n",
    "    text = re.sub(\"^[ \\t]+\", \"\", text, flags=re.MULTILINE)\n",
    "\n",
    "    # Remove trailing spaces before removing empty lines\n",
    "    text = re.sub(\"[ \\t]+$\", \"\", text, flags=re.MULTILINE)\n",
    "\n",
    "    # Remove empty lines\n",
    "    text = re.sub(\"^\\s+\", \"\", text, flags=re.MULTILINE)\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "#\n",
    "# Function to calculate the number of tokens in a text string.\n",
    "#\n",
    "\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "def num_tokens_from_string(string: str) -> int:\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get data files\n",
    "\n",
    "We load a number of HTML pages using the LangChain WebBaseLoader. Each of those pages contains lots of superfluous content so we extract only the relevant article context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Build a list of URLs to scrape from a text file.\n",
    "#\n",
    "\n",
    "# read urls from txt file\n",
    "with open('help_urls.txt') as f:\n",
    "    urls = f.readlines()\n",
    "\n",
    "# For debugging - Overide urls from text file \n",
    "#urls = [\"/help/personal/home-loans/apply-for-a-home-loan/choosing-which-home-loan-is-right-for-you.html\"]\n",
    "\n",
    "\n",
    "# remove whitespace characters like `\\n` at the end of each line\n",
    "urls = [x.strip() for x in urls]\n",
    "\n",
    "# prepend \"https://www.macquarie.com.au\" to each url\n",
    "urls = [\"https://www.macquarie.com.au\" + x for x in urls]\n",
    "\n",
    "display(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Scrape web data from the URLs\n",
    "#\n",
    "\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "import re\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "print(\"Loading Data\")\n",
    "url_loaders = WebBaseLoader(urls, header_template=headers)\n",
    "data = url_loaders.load()\n",
    "\n",
    "#\n",
    "# Extract only the actual Article content from the web page and clean\n",
    "#\n",
    "print(f\"Cleaning Data for {len(data)} documents\")\n",
    "\n",
    "for i, d in enumerate(data):\n",
    "    d.page_content = \"\"\n",
    "    source = d.metadata['source']\n",
    "    thedoc = WebBaseLoader(source, header_template=headers).scrape()\n",
    "    # extract only the Container Article content from the web page\n",
    "    td = thedoc.findAll('div', class_='parsys')\n",
    "    for t in td:\n",
    "        if len(t['class']) == 1 and t['class'][0] == 'parsys':\n",
    "            d.page_content = clean_text(t.text)\n",
    "            data[i] = d\n",
    "            # print i padded to 5 charactyers\n",
    "            print (f\"Doc: {i:3d}    Tokens: {num_tokens_from_string(d.page_content):6d}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the data into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "CHUNK_SIZE = 500\n",
    "\n",
    "# Chunk the data\n",
    "print(\"Splitting Data\")\n",
    "text_splitter = TokenTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=50)\n",
    "docs = text_splitter.split_documents(data)\n",
    "print(f\"Number of chunks: {len(docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store data in Astra Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "ASTRA_DB_APPLICATION_TOKEN = os.environ['ASTRA_DB_APPLICATION_TOKEN']\n",
    "OPENAI_API_KEY = os.environ['OPENAI_API_KEY']\n",
    "ASTRA_DB_API_ENDPOINT = os.environ['ASTRA_DB_API_ENDPOINT']\n",
    "ASTRA_DB_COLLECTION = \"mac_help\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding function\n",
    "\n",
    "Define the embedding model and create a function to generate vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "def embed(text_to_embed):\n",
    "    embedding = list(embeddings.embed_query(text_to_embed))\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialise Astra Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise Database\n",
    "\n",
    "from astrapy.db import AstraDB\n",
    "\n",
    "# Initialization\n",
    "db = AstraDB(\n",
    "  token=ASTRA_DB_APPLICATION_TOKEN,\n",
    "  api_endpoint=ASTRA_DB_API_ENDPOINT,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialise the Collection to use in Astra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Collection\n",
    "\n",
    "col = db.create_collection(ASTRA_DB_COLLECTION, dimension=1536)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assemble and Write data\n",
    "\n",
    "- Assemble chunks into JSON format, with the vector generated for each chunk\n",
    "- Store the chunks into Astra Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert Multiple Chunks\n",
    "\n",
    "# initialise documents list\n",
    "documents = []\n",
    "\n",
    "# iterate over chunks, calculate embedding, and append to documents list\n",
    "for i, d in enumerate(docs):\n",
    "    documents.append(\n",
    "        {\n",
    "            \"source\": d.metadata['source'],\n",
    "            \"title\": d.metadata['title'],\n",
    "            \"description\": d.metadata['description'],\n",
    "            \"language\": d.metadata['language'],\n",
    "            \"content\": d.page_content,\n",
    "            \"$vector\": embed(d.page_content)\n",
    "        }\n",
    "    )\n",
    "\n",
    "# insert documents list into collection\n",
    "res = col.insert_many(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_vector",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
